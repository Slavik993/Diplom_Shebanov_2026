# Mistral-7B LoRA Fine-Tuning –¥–ª—è SDGVM (–í–∏—Ç—Ç–µ)
# =============================================
# –≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è Google Colab (–±–µ—Å–ø–ª–∞—Ç–Ω—ã–π T4 GPU).
#
# –ò–ù–°–¢–†–£–ö–¶–ò–Ø:
# 1. –û—Ç–∫—Ä–æ–π—Ç–µ Google Colab: https://colab.research.google.com
# 2. –°–æ–∑–¥–∞–π—Ç–µ –Ω–æ–≤—ã–π –Ω–æ—É—Ç–±—É–∫
# 3. –í—ã–±–µ—Ä–∏—Ç–µ GPU: Runtime ‚Üí Change runtime type ‚Üí T4 GPU
# 4. –°–∫–æ–ø–∏—Ä—É–π—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –≤ —è—á–µ–π–∫–∏ –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –ø–æ –ø–æ—Ä—è–¥–∫—É
#
# –ü–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª witte_dataset.jsonl –≤ Colab
# (—á–µ—Ä–µ–∑ –±–æ–∫–æ–≤—É—é –ø–∞–Ω–µ–ª—å Files ‚Üí Upload)

# ============ –Ø–ß–ï–ô–ö–ê 1: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ ============
# !pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
# !pip install --no-deps trl peft accelerate bitsandbytes xformers

# ============ –Ø–ß–ï–ô–ö–ê 2: –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ ============
from unsloth import FastLanguageModel
import torch

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    max_seq_length=2048,
    dtype=None,  # –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (bfloat16 –Ω–∞ Ampere+, float16 –Ω–∞ T4)
    load_in_4bit=True,
)

print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

# ============ –Ø–ß–ï–ô–ö–ê 3: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ LoRA ============
model = FastLanguageModel.get_peft_model(
    model,
    r=16,                          # –†–∞–Ω–≥ LoRA
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                     "gate_proj", "up_proj", "down_proj"],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=42,
)

print("‚úÖ LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã –¥–æ–±–∞–≤–ª–µ–Ω—ã")
model.print_trainable_parameters()

# ============ –Ø–ß–ï–ô–ö–ê 4: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ ============
import json
from datasets import Dataset

# –ó–∞–≥—Ä—É–∂–∞–µ–º JSONL —Ñ–∞–π–ª
data = []
with open("witte_dataset.jsonl", "r", encoding="utf-8") as f:
    for line in f:
        entry = json.loads(line.strip())
        data.append(entry)

print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –∑–∞–ø–∏—Å–µ–π")

# –§–æ—Ä–º–∞—Ç –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è Mistral Instruct
def format_prompt(example):
    instruction = example.get("instruction", "")
    inp = example.get("input", "")
    output = example.get("output", "")
    
    if inp:
        text = f"""<s>[INST] {instruction}

{inp} [/INST] {output}</s>"""
    else:
        text = f"""<s>[INST] {instruction} [/INST] {output}</s>"""
    
    return {"text": text}

dataset = Dataset.from_list(data)
dataset = dataset.map(format_prompt)

print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω")
print(f"–ü—Ä–∏–º–µ—Ä: {dataset[0]['text'][:300]}...")

# ============ –Ø–ß–ï–ô–ö–ê 5: –û–±—É—á–µ–Ω–∏–µ ============
from trl import SFTTrainer
from transformers import TrainingArguments

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=2048,
    dataset_num_proc=2,
    packing=False,
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=5,
        num_train_epochs=3,           # 3 —ç–ø–æ—Ö–∏
        learning_rate=2e-4,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=10,
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="linear",
        seed=42,
        output_dir="outputs",
        save_strategy="epoch",
    ),
)

print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
stats = trainer.train()
print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
print(f"   –í—Ä–µ–º—è: {stats.metrics['train_runtime']:.0f} —Å–µ–∫—É–Ω–¥")
print(f"   Loss: {stats.metrics['train_loss']:.4f}")

# ============ –Ø–ß–ï–ô–ö–ê 6: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ============
FastLanguageModel.for_inference(model)

test_prompts = [
    "–†–∞—Å—Å–∫–∞–∂–∏ –æ –∑–æ–ª–æ—Ç–æ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–µ –í–∏—Ç—Ç–µ.",
    "–ö–∞–∫ —Å—Ç—Ä–æ–∏–ª—Å—è –¢—Ä–∞–Ω—Å—Å–∏–±?",
    "–ö–∞–∫–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è –±—ã–ª–∏ —É –í–∏—Ç—Ç–µ —Å –ù–∏–∫–æ–ª–∞–µ–º II?",
]

for prompt in test_prompts:
    inputs = tokenizer(f"<s>[INST] {prompt} [/INST]", return_tensors="pt").to("cuda")
    outputs = model.generate(**inputs, max_new_tokens=256, temperature=0.7)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"\n‚ùì {prompt}")
    print(f"üí¨ {response.split('[/INST]')[-1].strip()[:300]}")

# ============ –Ø–ß–ï–ô–ö–ê 7: –≠–∫—Å–ø–æ—Ä—Ç –≤ GGUF ============
# GGUF —Ñ–æ—Ä–º–∞—Ç –Ω—É–∂–µ–Ω –¥–ª—è LLMUnity
print("üì¶ –≠–∫—Å–ø–æ—Ä—Ç –≤ GGUF (Q4_K_M)...")
model.save_pretrained_gguf(
    "mistral-witte",
    tokenizer,
    quantization_method="q4_k_m"  # ~4 –ì–ë —Ñ–∞–π–ª
)

print("‚úÖ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω: mistral-witte-Q4_K_M.gguf")
print("üì• –°–∫–∞—á–∞–π—Ç–µ –µ–≥–æ –∏ –ø–æ–ª–æ–∂–∏—Ç–µ –≤ –ø–∞–ø–∫—É LLMUnity –≤–∞—à–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞")

# ============ –Ø–ß–ï–ô–ö–ê 8: –°–∫–∞—á–∏–≤–∞–Ω–∏–µ ============
# –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è:
# from google.colab import files
# files.download("mistral-witte-unsloth.Q4_K_M.gguf")

print("\nüéâ –ì–û–¢–û–í–û!")
print("–°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
print("1. –°–∫–∞—á–∞–π—Ç–µ —Ñ–∞–π–ª .gguf")
print("2. –ü–æ–ª–æ–∂–∏—Ç–µ –≤ –ø–∞–ø–∫—É LLMUnity –ø—Ä–æ–µ–∫—Ç–∞") 
print("3. –í Unity: LLMCharacter ‚Üí Model ‚Üí –≤—ã–±–µ—Ä–∏—Ç–µ –Ω–æ–≤—ã–π —Ñ–∞–π–ª")
